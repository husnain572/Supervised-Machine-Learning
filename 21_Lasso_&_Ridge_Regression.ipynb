{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOrQ7m6S1EaQPQ5QaKaC/mA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Understanding Lasso and Ridge Regression\n","\n","This document explains Lasso and Ridge Regression, including their theory, usage, and practical applications, along with answers to common questions.\n","\n","## What is Ridge Regression (L2 Regularization)?\n","\n","Ridge Regression is an improved version of Linear Regression that prevents overfitting by adding a penalty term. This penalty term is proportional to the square of the coefficients (L2 norm).\n","\n","- **Purpose**: To reduce the impact of unimportant features by shrinking the coefficients.\n","- **Key Idea**: It minimizes the sum of the squared residuals (errors) and adds a penalty proportional to the square of the coefficients.\n","- **Formula**:\n","  $$\n","  \\text{Loss} = \\text{Residual Sum of Squares (RSS)} + \\alpha \\sum_{j=1}^n \\beta_j^2\n","$$\n","  Where (alpha) is the regularization parameter, and (beta_j) are the coefficients of the features.\n","- **Effect**: All features are included, but their coefficients are shrunk towards zero. No coefficient becomes exactly zero.\n","\n","### Use Cases of Ridge Regression:\n","1. When all features are important, but their impact needs to be controlled.\n","2. When there is multicollinearity (high correlation between features).\n","3. When overfitting is a concern in linear regression models.\n","\n","## What is Lasso Regression (L1 Regularization)?\n","\n","Lasso Regression is another version of Linear Regression that adds a penalty proportional to the absolute value of the coefficients (L1 norm). It is widely used for feature selection.\n","\n","- **Purpose**: To reduce the number of irrelevant features by shrinking some coefficients to exactly zero.\n","- **Key Idea**: It minimizes the sum of the squared residuals (errors) and adds a penalty proportional to the absolute sum of the coefficients.\n","- **Formula**:\n","$$\n","  \\text{Loss} = \\text{Residual Sum of Squares (RSS)} + \\alpha \\sum_{j=1}^n |\\beta_j|\n","  $$\n","- **Effect**: Some feature coefficients become exactly zero, effectively excluding them from the model.\n","\n","### Use Cases of Lasso Regression:\n","1. When irrelevant features need to be removed automatically.\n","2. When feature selection is required in high-dimensional datasets.\n","3. To prevent overfitting while also simplifying the model.\n","\n","## Common Questions and Answers\n","\n","1. **Why do we use Ridge and Lasso Regression?**\n","   - To improve the performance of Linear Regression by preventing overfitting.\n","   - Ridge controls the magnitude of coefficients, while Lasso can eliminate irrelevant features entirely.\n","\n","2. **Where can we use Ridge Regression?**\n","   - Ridge Regression is useful in scenarios where all features are considered important, but their influence needs to be balanced. For example, predicting house prices using multiple correlated features like area, bedrooms, and location.\n","\n","3. **Where can we use Lasso Regression?**\n","   - Lasso Regression is ideal when feature selection is required. For instance, in a medical dataset with hundreds of variables, Lasso can identify the most critical variables affecting patient health.\n","\n","4. **How do we choose between Ridge and Lasso?**\n","   - Use Ridge when all features are important, and you want to control their impact.\n","   - Use Lasso when you suspect some features are irrelevant and want the model to exclude them automatically.\n","\n","5. **What is the role of the (alpha) parameter in both methods?**\n","   - The (alpha) parameter controls the strength of regularization. A higher (alpha) value increases the penalty, shrinking the coefficients more aggressively.\n","\n","6. **Can we use Ridge and Lasso together?**\n","   - Yes, Elastic Net combines Ridge and Lasso penalties, allowing both feature selection and coefficient shrinkage.\n","\n","## Steps to Use Ridge or Lasso Regression in Python\n","\n","1. **Prepare the Dataset:**\n","   - Collect data and clean it (handle missing values, normalize features if necessary).\n","2. **Split the Dataset:**\n","   - Divide the data into training and testing sets (e.g., 80% training, 20% testing).\n","3. **Fit the Model:**\n","   - Use Ridge or Lasso from Pythonâ€™s `sklearn` library to train the model on the training dataset.\n","4. **Evaluate the Model:**\n","   - Test the model on unseen data (testing set) and check its performance using metrics like Mean Squared Error (MSE).\n","\n","## Conclusion\n","\n","- **Ridge Regression** is best for handling multicollinearity and controlling coefficient sizes when all features are relevant.\n","- **Lasso Regression** is best for feature selection when irrelevant features are present.\n","- Both methods enhance the performance of Linear Regression and make it more robust for real-world applications.\n"],"metadata":{"id":"rz83tj2fbZRS"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7eWS9C2mbSmo","executionInfo":{"status":"ok","timestamp":1739385290959,"user_tz":-300,"elapsed":3120,"user":{"displayName":"Muhammad Husnain","userId":"06157149168527424812"}},"outputId":"d33bdffc-81ad-432b-8a2a-49ed2bd93de5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ridge MSE: 459802525.60451496\n","Lasso MSE: 534638903.95028365\n","Ridge Coefficients: [ 112.41263586  -46.80416493 -257.59942843  988.15505279]\n","Lasso Coefficients: [  115.62352921  -669.34528569 -2036.99351894  1020.6422698 ]\n"]}],"source":["from sklearn.linear_model import Ridge, Lasso\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import pandas as pd\n","\n","data = {\n","    \"area\": [1200, 1500, 2000, 2500, 3000],\n","    \"bedrooms\": [2, 3, 3, 4, 5],\n","    \"bathrooms\": [1, 2, 2, 3, 3],\n","    \"age\": [10, 5, 20, 15, 8],\n","    \"price\": [200000, 250000, 300000, 350000, 400000]\n","}\n","df = pd.DataFrame(data)\n","\n","# Features (X) and Target (y)\n","X = df[[\"area\", \"bedrooms\", \"bathrooms\", \"age\"]]\n","y = df[\"price\"]\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Ridge Regression\n","ridge = Ridge(alpha=1.0)\n","ridge.fit(X_train, y_train)\n","ridge_predictions = ridge.predict(X_test)\n","\n","# Lasso Regression\n","lasso = Lasso(alpha=0.1)\n","lasso.fit(X_train, y_train)\n","lasso_predictions = lasso.predict(X_test)\n","\n","# Evaluate Models\n","print(\"Ridge MSE:\", mean_squared_error(y_test, ridge_predictions))\n","print(\"Lasso MSE:\", mean_squared_error(y_test, lasso_predictions))\n","\n","# Check coefficients\n","print(\"Ridge Coefficients:\", ridge.coef_)\n","print(\"Lasso Coefficients:\", lasso.coef_)\n","\n"]}]}