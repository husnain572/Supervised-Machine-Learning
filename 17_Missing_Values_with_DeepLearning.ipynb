{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrnSnh1Q91bD0oS07ss6Mw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Deep Learning Methods for Missing Value Imputation**\n","\n","Deep learning techniques, particularly **neural networks** such as **autoencoders**, provide an effective way to impute missing values in complex datasets. These methods are especially beneficial when data exhibits **non-linear relationships** that traditional imputation techniques struggle to handle.\n","\n","---\n","\n","## **Understanding Autoencoders for Imputation**\n","\n","### **What is an Autoencoder?**\n","An **autoencoder** is a type of neural network designed to **reconstruct its input**. It consists of two main components:  \n","- **Encoder**: Compresses the input into a **latent-space representation**.  \n","- **Decoder**: Reconstructs the original input from this compressed representation.  \n","\n","This architecture allows autoencoders to learn a meaningful representation of data, making them useful for missing value imputation.\n","\n","---\n","\n","## **How Autoencoders Work for Imputation**\n","1. **Training with Incomplete Data**  \n","   - The model is trained with **incomplete inputs**, allowing it to learn to **ignore missing values**.  \n","   \n","2. **Predicting Missing Values**  \n","   - The autoencoder minimizes the **reconstruction error** for the known data, enabling it to **predict missing values** accurately.  \n","\n","3. **Learning Robust Representations**  \n","   - By capturing the underlying patterns, autoencoders **generate reasonable approximations** for missing values.\n","\n","---\n","\n","## **Advantages of Using Autoencoders**\n","**Captures Complex Patterns**: Can model **non-linear relationships** in data.  \n","**Scalability**: Efficiently handles **large datasets**.  \n","**Flexibility**: Works with **different data types** (images, text, time-series, tabular data).  \n","\n","---\n","\n","## **Key Considerations for Implementation**\n","- **Data Preprocessing**: Inputs should be **normalized or standardized** before training.  \n","- **Network Architecture**: The **number and type of layers** should match the complexity of the dataset.  \n","- **Training Techniques**: Methods like **dropout** and **noise addition** can improve imputation performance.  \n","\n","---\n","\n","## **Example Use Cases**\n","**Image Data** → Reconstructs missing pixels or repairs corrupted images.  \n","**Time-Series Data** → Fills in missing values in sequential data (e.g., stock prices, weather patterns).  \n","**Tabular Data** → Handles missing values in structured datasets used for machine learning.  \n","\n","---\n","\n","### **Summary**\n","Autoencoders provide a **powerful deep learning-based approach** for imputing missing values by **learning robust data representations**. Their ability to **capture complex relationships** makes them ideal for handling large and diverse datasets across various domains.\n","\n"],"metadata":{"id":"iYCH66QcJHtA"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZIyKNJAXI8tb","executionInfo":{"status":"ok","timestamp":1739381086641,"user_tz":-300,"elapsed":9161,"user":{"displayName":"Muhammad Husnain","userId":"06157149168527424812"}},"outputId":"b214c058-2298-4a34-a15c-ec1b1c4c3731"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 0.2452 - val_loss: 0.2438\n","Epoch 2/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2401 - val_loss: 0.2392\n","Epoch 3/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2359 - val_loss: 0.2347\n","Epoch 4/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2307 - val_loss: 0.2302\n","Epoch 5/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2279 - val_loss: 0.2259\n","Epoch 6/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2225 - val_loss: 0.2216\n","Epoch 7/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2183 - val_loss: 0.2174\n","Epoch 8/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2141 - val_loss: 0.2132\n","Epoch 9/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2101 - val_loss: 0.2090\n","Epoch 10/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2059 - val_loss: 0.2049\n","Epoch 11/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2016 - val_loss: 0.2007\n","Epoch 12/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1978 - val_loss: 0.1965\n","Epoch 13/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1937 - val_loss: 0.1923\n","Epoch 14/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1898 - val_loss: 0.1880\n","Epoch 15/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1852 - val_loss: 0.1836\n","Epoch 16/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1810 - val_loss: 0.1793\n","Epoch 17/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1765 - val_loss: 0.1749\n","Epoch 18/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1719 - val_loss: 0.1704\n","Epoch 19/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1683 - val_loss: 0.1660\n","Epoch 20/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1642 - val_loss: 0.1615\n","Epoch 21/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1588 - val_loss: 0.1570\n","Epoch 22/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1548 - val_loss: 0.1525\n","Epoch 23/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1501 - val_loss: 0.1480\n","Epoch 24/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1458 - val_loss: 0.1435\n","Epoch 25/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1413 - val_loss: 0.1390\n","Epoch 26/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1368 - val_loss: 0.1345\n","Epoch 27/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1330 - val_loss: 0.1301\n","Epoch 28/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1287 - val_loss: 0.1258\n","Epoch 29/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1242 - val_loss: 0.1215\n","Epoch 30/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1199 - val_loss: 0.1174\n","Epoch 31/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1157 - val_loss: 0.1134\n","Epoch 32/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1125 - val_loss: 0.1095\n","Epoch 33/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1086 - val_loss: 0.1058\n","Epoch 34/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1044 - val_loss: 0.1021\n","Epoch 35/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1011 - val_loss: 0.0986\n","Epoch 36/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0976 - val_loss: 0.0952\n","Epoch 37/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0945 - val_loss: 0.0919\n","Epoch 38/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0915 - val_loss: 0.0888\n","Epoch 39/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0877 - val_loss: 0.0857\n","Epoch 40/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0857 - val_loss: 0.0829\n","Epoch 41/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0825 - val_loss: 0.0802\n","Epoch 42/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0800 - val_loss: 0.0776\n","Epoch 43/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0776 - val_loss: 0.0752\n","Epoch 44/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0753 - val_loss: 0.0729\n","Epoch 45/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0720 - val_loss: 0.0707\n","Epoch 46/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0700 - val_loss: 0.0686\n","Epoch 47/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0685 - val_loss: 0.0666\n","Epoch 48/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0668 - val_loss: 0.0647\n","Epoch 49/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0650 - val_loss: 0.0628\n","Epoch 50/50\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0635 - val_loss: 0.0611\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n"]}],"source":["import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","\n","# Load the Titanic dataset\n","df_titanic = sns.load_dataset('titanic')\n","\n","# Selecting relevant features for simplicity\n","df_titanic = df_titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n","\n","# Preprocessing\n","# Separate features and target\n","X = df_titanic.drop('survived', axis=1)\n","y = df_titanic['survived']\n","\n","# Handling missing values and categorical variables\n","numeric_features = ['age', 'fare', 'sibsp', 'parch']\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='mean')),\n","    ('scaler', MinMaxScaler())])\n","\n","categorical_features = ['pclass', 'sex', 'embarked']\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n","\n","# ColumnTransformer for preprocessing\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)])\n","\n","# Preprocessing the dataset\n","X_preprocessed = preprocessor.fit_transform(X)\n","\n","# Splitting the dataset (we'll use the train set to train the autoencoder)\n","X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n","\n","# Define the autoencoder architecture\n","input_dim = X_train.shape[1]\n","encoding_dim = 32\n","\n","input_layer = Input(shape=(input_dim,))\n","encoded = Dense(encoding_dim, activation='relu')(input_layer)\n","decoded = Dense(input_dim, activation='sigmoid')(encoded)\n","\n","autoencoder = Model(input_layer, decoded)\n","autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the autoencoder\n","autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n","\n","# Using the autoencoder for imputation on test set\n","X_test_imputed = autoencoder.predict(X_test)\n","\n","# Note: Transforming imputed data back to original feature space is complex and requires reversing the preprocessing steps.\n","# This is often not straightforward, especially for one-hot encoded features.\n"]}]}